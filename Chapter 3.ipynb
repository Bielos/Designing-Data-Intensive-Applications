{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Storage and Retrieval\n",
    "Why should you, as an application developer, care how the database handles storage\n",
    "and retrieval internally? You’re probably not going to implement your own storage\n",
    "engine from scratch, but you do need to select a storage engine that is appropriate for\n",
    "your application, from the many that are available. In order to tune a storage engine\n",
    "to perform well on your kind of workload, you need to have a rough idea of what the\n",
    "storage engine is doing under the hood.\n",
    "\n",
    "There are two families of storage engines: **log-structured storage engines**, and **page-oriented storage engines**\n",
    "such as B-trees.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-structured storage engines\n",
    "Storage engines that uses an append-only\n",
    "sequence of records (called _log_). It doesn’t have to be human-readable; it might\n",
    "be binary and intended only for other programs to read. \n",
    "\n",
    "This engine **is optimized for faster write** but has terrible performance if you have a large number of records in your database. to increase read performance databases uses an **index** wich is an additional structure that is derived from the primary data. Many databases\n",
    "allow you to add and remove indexes, and this doesn’t affect the contents of the\n",
    "database; it only affects the performance of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hash Indexes\n",
    "Let’s say our data storage consists only of appending to a file, as in the preceding\n",
    "example. Then the simplest possible indexing strategy is this: keep an in-memory\n",
    "hash map where every key is mapped to a byte offset in the data file—the location at\n",
    "which the value can be found. When you want to look up a value, use the hash map to find the offset in the\n",
    "data file, seek to that location, and read the value.\n",
    "\n",
    "![](images/image_6.png)\n",
    "\n",
    "As described so far, we only ever append to a file—so how do we avoid eventually\n",
    "running out of disk space? A good solution is to break the log into segments of a certain\n",
    "size by closing a segment file when it reaches a certain size, and making subsequent\n",
    "writes to a new segment file. We can then perform compaction on these\n",
    "segments. Compaction means throwing away duplicate\n",
    "keys in the log, and keeping only the most recent update for each key.\n",
    "\n",
    "![](images/image_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations and issues in the real world**\n",
    "* File format\n",
    "    - It’s faster and simpler to use a binary format that first encodes the length of a string in bytes, followed by the raw string\n",
    "* Deleting records\n",
    "    - If you want to delete a key and its associated value, you have to append a special\n",
    "deletion record to the data file (sometimes called a tombstone). When log segments\n",
    "are merged, the tombstone tells the merging process to discard any previous\n",
    "values for the deleted key.\n",
    "* Crash recovery\n",
    "    - If the database is restarted, the in-memory hash maps are lost. In principle, you\n",
    "can restore each segment’s hash map by reading the entire segment file from\n",
    "beginning to end and noting the offset of the most recent value for every key as\n",
    "you go along. However, that might take a long time if the segment files are large,\n",
    "which would make server restarts painful\n",
    "* Partially written records\n",
    "    - The database may crash at any time, including halfway through appending a\n",
    "record to the log. Bitcask files include checksums, allowing such corrupted parts\n",
    "of the log to be detected and ignored.\n",
    "* Concurrency control\n",
    "    - As writes are appended to the log in a strictly sequential order, a common implementation\n",
    "choice is to have only one writer thread. Data file segments are\n",
    "append-only and otherwise immutable, so they can be read concurrently by multiple\n",
    "threads.\n",
    "* The hash table must fit in memory, so if you have a very large number of keys,\n",
    "you’re out of luck.\n",
    "* Range queries are not efficient. For example, you cannot easily scan over all keys\n",
    "between kitty00000 and kitty99999—you’d have to look up each key individually\n",
    "in the hash maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSTables and LSM-Trees\n",
    "Now we can make a simple change to the format of our segment files: we require that\n",
    "the sequence of key-value pairs is sorted by key. We call this format Sorted String Table, or **SSTable** for short.\n",
    "\n",
    "**Advantages**\n",
    "* Merging segments is simple and efficient, even if the files are bigger than the\n",
    "available memory.\n",
    "\n",
    "![](images/image_8.png)\n",
    "\n",
    "* In order to find a particular key in the file, you no longer need to keep an index\n",
    "of all the keys in memory.\n",
    "\n",
    "Example: say you’re looking for\n",
    "the key handiwork, but you don’t know the exact offset of that key in the segment\n",
    "file. However, you do know the offsets for the keys handbag and handsome, and\n",
    "because of the sorting you know that handiwork must appear between those two.\n",
    "This means you can jump to the offset for handbag and scan from there until you\n",
    "find handiwork (or not, if the key is not present in the file).\n",
    "\n",
    "![](images/image_9.png)\n",
    "\n",
    "* Since read requests need to scan over several key-value pairs in the requested\n",
    "range anyway, it is possible to group those records into a block and compress it\n",
    "before writing it to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read and write process**\n",
    "* When a write comes in, add it to an in-memory balanced tree data structure (for\n",
    "example, a red-black tree). This in-memory tree is sometimes called a memtable.\n",
    "* When the memtable gets bigger than some threshold—typically a few megabytes\n",
    "—write it out to disk as an SSTable file. This can be done efficiently because the\n",
    "tree already maintains the key-value pairs sorted by key. The new SSTable file\n",
    "becomes the most recent segment of the database. While the SSTable is being\n",
    "written out to disk, writes can continue to a new memtable instance.\n",
    "* In order to serve a read request, first try to find the key in the memtable, then in\n",
    "the most recent on-disk segment, then in the next-older segment, etc.\n",
    "* From time to time, run a merging and compaction process in the background to\n",
    "combine segment files and to discard overwritten or deleted values.\n",
    "\n",
    "Originally this indexing structure was described by Patrick O’Neil et al. under the\n",
    "name _Log-Structured Merge-Tree_ (or **LSM-tree**).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page-oriented storage engines\n",
    "#### B-Trees\n",
    "The log-structured indexes we saw earlier break the database down into variable-size\n",
    "segments, typically several megabytes or more in size, and always write a segment\n",
    "sequentially. By contrast, B-trees break the database down into fixed-size blocks or\n",
    "pages, traditionally 4 KB in size (sometimes bigger), and read or write one page at a\n",
    "time. This design corresponds more closely to the underlying hardware, as disks are\n",
    "also arranged in fixed-size blocks. \n",
    "\n",
    "Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient keyvalue\n",
    "lookups and range queries. But that’s where the similarity ends.\n",
    "\n",
    "![](images/image_10.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Processing vs Analytics\n",
    "An application\n",
    "typically looks up a small number of records by some key, using an index. Records\n",
    "are inserted or updated based on the user’s input. Because these applications are\n",
    "interactive, the access pattern became known as **online transaction processing\n",
    "(OLTP)**. However, databases also started being increasingly used for data analytics, which has\n",
    "very different access patterns.In order\n",
    "to differentiate this pattern of using databases from transaction processing, it has\n",
    "been called **online analytic processing (OLAP)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Warehousing\n",
    "A **data warehouse** is a separate database that analysts can query to their\n",
    "hearts’ content, without affecting OLTP operations. The data warehouse contains\n",
    "a read-only copy of the data in all the various OLTP systems in the company.\n",
    "Data is extracted from OLTP databases (using either a periodic data dump or a continuous\n",
    "stream of updates), transformed into an analysis-friendly schema, cleaned\n",
    "up, and then loaded into the data warehouse. This process of getting data into the\n",
    "warehouse is known as Extract–Transform–Load (ETL).\n",
    "\n",
    "![](images/image_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data warehouse data models\n",
    "Many data warehouses\n",
    "are used in a fairly formulaic style, known as a star schema (also known as dimensional\n",
    "modeling, composed of a primary _fact table_ and related dimensions) and a variation of this template known as the snowflake schema, where dimensions are\n",
    "further broken down into subdimensions.\n",
    "\n",
    "![](images/image_12.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-Oriented Storage\n",
    "The idea behind column-oriented storage is simple: don’t store all the values from one\n",
    "row together, but store all the values from each column together instead. If each column\n",
    "is stored in a separate file, a query only needs to read and parse those columns\n",
    "that are used in that query, which can save a lot of work.\n",
    "\n",
    "![](images/image_13.png)\n",
    "![](images/image_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cubes and Materialized Views\n",
    "OLAP systems are frequently used for aggregation queries (COUNT, SUM, AVG, MIN, MAX). One way of caching those queries is a **materialized view**. In a relational data model, it\n",
    "is often defined like a standard (virtual) view: a table-like object whose contents are\n",
    "the results of some query. The difference is that a materialized view is an actual copy\n",
    "of the query results, written to disk, whereas a virtual view is just a shortcut for writing\n",
    "queries.\n",
    "\n",
    "A common special case of a materialized view is known as a data cube or OLAP cube. It is a grid of aggregates grouped by different dimensions.\n",
    "\n",
    "![](images/image_15.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
