{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Data-Intensive Applications: Step-by-step guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Big picture of components (See chapter 1)\n",
    "The first step in designing a data-intensive application is to broadly identify the components that will come into play. You can ask yourself questions like:\n",
    "* How big is my data?\n",
    "* How complex is it?\n",
    "* How fast doest it change?\n",
    "* Does it need caching processes?\n",
    "* Does it need to allow users to search data by keyword or filter it in various ways (search indexes)?\n",
    "\n",
    "The idea is to clarify a broad picture of the components that are needed in the architecture, and then go deeper into each component by selecting the technology that best fits the overall need of the application.\n",
    "\n",
    "![](images/image_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Designing for reliability (See chapter 1)\n",
    "The idea is to choose an initial (it may change at any stage of the design process) prevention strategy for:\n",
    "* Hardare faults\n",
    "    - Adding redundancy on hardware components \n",
    "    - Using systems that can tolerate the loss of an entire machine (distributed systems)\n",
    "* Software errors\n",
    "    - Check software assumptions and interactions\n",
    "    - Testing\n",
    "    - Monitoring of software designed alerts about assumptions and interactions\n",
    "* Human errors\n",
    "    - Well designed abstractions\n",
    "    - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Designing for scalability (See chapter 1)\n",
    "Scalability is the term we use to describe a systemâ€™s ability to cope with increased load. It includes questions like:\n",
    "* If the system grows in a particular way, what are our options for coping with the growth?\n",
    "* How can we add computing resources to handle the additional load?\n",
    "\n",
    "In this step you must identify the following parameters (examples not exclusive):\n",
    "* Load parameter\n",
    "    - Requests per second to a web server\n",
    "    - Ratio of reads to writes in a database\n",
    "    - Number of simultaneously active users in a chat room\n",
    "    - Hit rate on a cache\n",
    "* Performance parameter\n",
    "    - Throughput (distributed systems)\n",
    "    - Response time (online systems)\n",
    "    \n",
    "Based on the parameters the architect choose the correct scalability strategy to deal with the additional load so it do not impact the performance. (i.e. manual scaling vs _elastic_ systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Designing for maintainability (See chapter 1)\n",
    "The idea is to choose an strategy for better:\n",
    "* Operability\n",
    "    - Good monitoring\n",
    "    - Good support for automation and integration with standard tools\n",
    "    - Avoiding dependency on individual machines\n",
    "    - Good documentation\n",
    "* Simplicity\n",
    "    - Good abstraction\n",
    "* Evolvability\n",
    "    - Agile working patterns\n",
    "    - Test-driven development (TDD)\n",
    "    - Refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Choosing a data model (See chapter 2)\n",
    "\n",
    "Depending on the number of relationships that exists in the abstraction, the most popular data models for databases are relational, document and graphs.\n",
    "\n",
    "* Relational databases fits in the majority of cases.\n",
    "* Document databases target use cases where data comes in self-contained documents\n",
    "and relationships between one document and another are rare.\n",
    "* Graph databases go in the opposite direction, targeting use cases where anything\n",
    "is potentially related to everything.\n",
    "\n",
    "A feature to take into account is also the query language used for each model. For the relational model **the schema is explicit** (enforced on write) contrary to document and graph where **the schema is implicit** (handled on read).  \n",
    "\n",
    "If the data to store need to be a binary object (text, video, JSON document, etc.) a key-value database will be the best choise, since they are highly effective at scaling applications that deal with high-velocity, non-transactional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Choosing a storage processing systems (See chapter 3)\n",
    "Depending on the read and write patterns of the application an OLTP or an OLAP system should be used.\n",
    "\n",
    "![](images/image_16.png)\n",
    "\n",
    "For OLTP systems the storage engines are devided into log-structured and page-oriented. \n",
    "\n",
    "* Log-structured\n",
    "    - Hash index: Rask\n",
    "    - LSM-Tree: Cassandra, HBase\n",
    "    - Search engines: Lucene\n",
    "* Page-oriented\n",
    "    - B-Tree: LMDB\n",
    "    - B+ Tree: SQL Server, Oracle\n",
    "\n",
    "As a rule of thumb, LSM-trees are typically faster for writes, whereas B-trees\n",
    "are thought to be faster for reads. Reads are typically slower on LSM-trees\n",
    "because they have to check several different data structures and SSTables at different\n",
    "stages of compaction.\n",
    "\n",
    "For OLAP systems the most important factor is the data model, here Star and Snowflake are the options. Disk bandwidth (not seek time) is often the bottleneck here, and column-oriented storage is an increasingly popular solution for this kind of workload.\n",
    "\n",
    "* Column-oriented products\n",
    "    - Apache Parquet\n",
    "* Datawarehouse products\n",
    "    - OLTP and OLAP: SAP HANA, SQL Server\n",
    "    - OLAP (paid) --> Teradata, Vertica, Amazon RedShift, Google BigQuery\n",
    "    - OLAP (open source) --> Apache Hive, Spark SQL, Impala, Presto, Apache Tajo, Apache Drill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
